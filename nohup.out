/root/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
wandb: Currently logged in as: balcells-oscar. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.2
wandb: Run data is saved locally in /root/sae/wandb/run-20240709_083900-p9br7drb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-moon-16
wandb: ⭐️ View project at https://wandb.ai/balcells-oscar/sae
wandb: 🚀 View run at https://wandb.ai/balcells-oscar/sae/runs/p9br7drb
Average token length: 209.09
Optimal batch size: 78
Training on layers: [12]
Loaded SAEs from disk for layers: [12]
Auto-selected LR: 7.07e-05
Using 8-bit Adam from bitsandbytes
Number of SAE parameters: 1_073_876_992
Number of model parameters: 8_030_261_248
Training:   0%|          | 0/3008 [00:00<?, ?it/s]Training:   0%|          | 1/3008 [00:02<2:15:31,  2.70s/it]Training:   0%|          | 2/3008 [00:04<1:59:52,  2.39s/it]Training:   0%|          | 3/3008 [00:07<1:54:43,  2.29s/it]Training:   0%|          | 4/3008 [00:09<1:52:20,  2.24s/it]Training:   0%|          | 5/3008 [00:11<1:51:02,  2.22s/it]Training:   0%|          | 6/3008 [00:13<1:50:41,  2.21s/it]Training:   0%|          | 7/3008 [00:15<1:49:58,  2.20s/it]Training:   0%|          | 8/3008 [00:17<1:49:28,  2.19s/it]Training:   0%|          | 9/3008 [00:20<1:48:43,  2.18s/it]